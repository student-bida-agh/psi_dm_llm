# Useful resources  

Here are some helpful resources I used while making this course. Iâ€™m sharing them with you in case they help. Enjoy!  

## Videos  

- [Deep Dive into LLMs like ChatGPT (Theory)](https://www.youtube.com/watch?v=7xTGNNLPyMI&t=7652s)  
- [Building GPT from Scratch (Practice 1)](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=1s)  
- [Building a GPT Tokenizer (Practice 2)](https://www.youtube.com/watch?v=zduSFxRajkE&t=440s)  
- [Reproducing GPT-2 (Practice 3)](https://www.youtube.com/watch?v=l8pRSuU81PU)  

## GitHub repositories  

- [Train a language model using WhatsApp group chats](https://github.com/bernhard-pfann/lad-gpt)  
- [Create an AI version of yourself using WhatsApp chats](https://github.com/kinggongzilla/ai-clone-whatsapp)  
- [Extract WhatsApp key/database without root access](https://github.com/YuvrajRaghuvanshiS/WhatsApp-Key-Database-Extractor)  

## Articles  

- [Train a language model on your WhatsApp chats](https://towardsdatascience.com/build-a-language-model-on-your-whatsapp-chats-31264a9ced90/)  
- [Fine-tune an LLM to create your digital twin](https://medium.com/better-programming/unleash-your-digital-twin-how-fine-tuning-llm-can-create-your-perfect-doppelganger-b5913e7dda2e)  
- [Fine-tuning LLMs using QLoRA](https://dassum.medium.com/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07)  
- [Guide to fine-tuning large language models](https://www.datacamp.com/tutorial/fine-tuning-large-language-models)  
- [Understanding LoRA and DoRA for model tuning](https://magazine.sebastianraschka.com/p/lora-and-dora-from-scratch)  
- [Step-by-step guide to fine-tune LLMs with LoRA](https://medium.com/@manindersingh120996/practical-guide-to-fine-tune-llms-with-lora-c835a99d7593)  
- [Fine-tuning LLMs for multi-turn conversations](https://www.together.ai/blog/fine-tuning-llms-for-multi-turn-conversations-a-technical-deep-dive#:~:text=Fine-tuning%20LLMs%20for%20multi-turn%20conversations%20requires%20careful%20attention,while%20managing%20computational%20resources%20efficiently.)  
- [Fine-tuning mixtral 7bx8 with LoRA](https://medium.com/@prakharsaxena11111/finetuning-mixtral-7bx8-6071b0ebf114)
- [Positional Encoding Explained: A Deep Dive into Transformer PE](https://medium.com/thedeephub/positional-encoding-explained-a-deep-dive-into-transformer-pe-65cfe8cfe10b)
- [You could have designed state of the art positional encoding](https://huggingface.co/blog/designing-positional-encoding)
- [Relative Positional Encoding](https://jaketae.github.io/study/relative-positional-encoding/)

## Reddit discussions  

- [Finetuned Llama 2-7B using WhatsApp chats](https://www.reddit.com/r/LocalLLaMA/comments/18ny05c/finetuned_llama_27b_on_my_whatsapp_chats/)  
- [How to train your model](https://www.reddit.com/r/Oobabooga/comments/19480dr/how_to_train_your_dra_model/?share_id=FandRNmK84MItOJYIynap&utm_medium=android_app&utm_name=androidcss&utm_source=share&utm_term=1)  
- [Exporting full WhatsApp chat history](https://www.reddit.com/r/DataHoarder/comments/a7c0yq/full_whatsapp_chat_export_40000_messages/)  

## Notebooks  

- [Unsloth AI Notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks)  
- [LLMs-from-Scratch: Chapter 6 Notebook](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch06/01_main-chapter-code/ch06.ipynb?utm_source=substack&utm_medium=email)  
- [LoRA Implementation from Scratch](https://www.kaggle.com/code/aisuko/lora-from-scratch)  
