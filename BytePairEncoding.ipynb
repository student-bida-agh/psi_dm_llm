{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGiKPfxmA9kg"
      },
      "source": [
        "## Load the sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_jon1cpA9kg"
      },
      "outputs": [],
      "source": [
        "with open(\"../output/combined_text.txt\", \"r\") as f:\n",
        "    text_sequence = f.read()\n",
        "\n",
        "len(text_sequence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIt5F00JA9kh"
      },
      "source": [
        "## BPE algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdIfglyOA9kh"
      },
      "source": [
        "I am using the [minBPE](https://github.com/karpathy/minbpe) repository to tokenize the sequence of text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Waypz2QpA9kh"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('..')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS-lpdjZA9kh"
      },
      "source": [
        "Start by training the tokenizer on the text sequence that you saved in the previous notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6f6Hs8cA9kh"
      },
      "outputs": [],
      "source": [
        "from minbpe import BasicTokenizer\n",
        "\n",
        "tokenizer = BasicTokenizer()\n",
        "tokenizer.train(text_sequence, vocab_size=1024)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMKR-fSgA9ki"
      },
      "source": [
        "Visualize the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvWnNntVA9ki"
      },
      "outputs": [],
      "source": [
        "vocab = tokenizer.vocab\n",
        "vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rO_Wc9nMA9ki"
      },
      "source": [
        "Test the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDh-xbEkA9ki"
      },
      "outputs": [],
      "source": [
        "tokenizer.encode(\"Salam labas\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yb2KqKsPA9kj",
        "outputId": "d2812430-4943-4172-be07-7deabc3d6c9f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Salam labas'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode([702, 310, 346, 115])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWhrc2DDA9kj"
      },
      "source": [
        "Add special tokens to the vocabulary. These tokens are going to be used a lot in the fine-tuning step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBuqVHC7A9kj"
      },
      "outputs": [],
      "source": [
        "max_vocab_id = list(tokenizer.vocab.keys())[-1]\n",
        "tokenizer.special_tokens = {\n",
        "    \"<|startoftext|>\": max_vocab_id + 1,\n",
        "    \"<|separator|>\": max_vocab_id + 2,\n",
        "    \"<|endoftext|>\": max_vocab_id + 3,\n",
        "    \"<|unk|>\": max_vocab_id + 4\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PxjaDeDA9kj"
      },
      "source": [
        "I have more than 618K tokens for training and validation. This is pretty good, but if you can add more, that would be even better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THrrIGtVA9kj"
      },
      "outputs": [],
      "source": [
        "len(tokenizer.encode(text_sequence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IDWau3lA9kj"
      },
      "source": [
        "Save the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nw4B6iESA9kj"
      },
      "outputs": [],
      "source": [
        "tokenizer.save(file_prefix=\"../output/tokenizer/my_tokenizer\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "vincent",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}